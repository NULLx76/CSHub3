<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>MyBlog</title>
  <link rel="stylesheet" href="https://nullx76.github.io/CSHub3/main.css">
</head>

<body>
  <div class="container">
    <head>
      <h1><a href="/">CSHub.nl</a></h1>
    </head>
    <section class="section">
      
  <section class="page">
  <h2 id="static-vs-dynamic">Static vs dynamic</h2><p><strong>Static analysis: </strong>testing of a component or system at specification or implementation level without execution of software (e.g. reviews)</p><p><strong>Dynamic testing: </strong>testing that involves the execution of the software of a component or system</p><p><br></p><h2 id="reviews">Reviews</h2><p>Reviews can range from very informal to formal. In practice, an informal review is most often what occurs.</p><p><br></p><p>The formal review consists of the following stages:</p><ul><li><strong>Planning</strong>: the <strong>author </strong>requests a review to the <strong>moderator</strong> (leading the review process) and then entry (to determine whether it's worth to review) and exit criteria (to determine when the review is done) are determined</li><li><strong>Kick-off: </strong>this meeting is to get everyone on the same level and to assign roles</li><li><strong>Preparation: </strong>the <strong>reviewers </strong>individually review it</li><li><strong>Review meeting: </strong>defects are discussed and given a severity level and based on that a decision is made. The <strong>scribe </strong>records each mentioned defect.</li><li><strong>Rework: </strong>based on the defects, the author will improve </li><li><strong>Follow-up: </strong>check if everything that had to be changed is changed</li></ul><p><br></p><h3 id="types-of-review">Types of review</h3><ul><li><strong>Informal: </strong>undocumented reviews where the author asks for review</li><li><strong>Walk-through</strong>: author in the lead, tells the reviewers their though process.</li><li><strong>Technical review</strong>: technical meeting to achieve consensus</li><li><strong>Inspection</strong>: peer review of documents, visual inspection to detect defects and violations, after that a meeting takes place discussing the found defects</li></ul><p><br></p><h2 id="static-analysis-by-tools">Static analysis by tools</h2><p>Code review by computers, checks all possible code paths. It is relatively easy to extract results, but with limited capability.  Certain metrics can be used to find those defects, for example using <strong>cyclomatic complexity. </strong></p><p><br></p><p>Static analysis tools try to be the following (it is impossible to accomplish everything)</p><ul><li><strong>Soundness</strong>, no missed bugs or vulnerabilities (but we can have false alarms)</li><li><strong>Completeness / preciseness</strong>, no false alarms (but we can miss bugs)</li><li><strong>Interpretation</strong>, do you understand the warnings?</li><li><strong>Scalable</strong>, is it scalable to bigger applications?</li></ul><p><br></p><p>There are a few types of these tools:</p><h3 id=""><br></h3><h3 id="lexical"><strong>Lexical</strong></h3><p>Look for predefined patterns in code using:</p><ul><li>Regular expressions</li><li>Finite state automata</li></ul><p><br></p><p>Not the best solution for everything as many things aren't always incorrect</p><p><br></p><h3 id="syntactic-analysis">Syntactic analysis</h3><p>Performed through parsers, which convert unstructured source code into a hierarchical data structure. These are then converted into an <strong>abstract syntax tree (AST)</strong>, which basically shows in which order what operation which be executed.</p><p><br></p><p>We then check some rules against this AST, allowing for more fine testing (as opposed to pattern matching).</p><p><br></p><h3 id="control-flow-graph">Control flow graph</h3><p>It checks all execution paths a program <em>might</em> take, tracing execution with actually executing the program. Using this we can detect unreachable code. </p><p><br></p><p>We can also use <strong>data flow analysis </strong>which tracks all values variables <em>might </em>have, track how and when it's viewed and modified. We make a distinction between <strong>tainted</strong> (that can be modified by the user) data and <strong>untainted</strong> (supplied by the application) data. We have to prove that no untainted data is expected and no tainted data is used. For example: we should make sure no tainted data is executed. This is especially important in security testing.</p><h3 id=""><br></h3><h3 id="symbolic-execution">Symbolic execution</h3><p>Looking at what different types of inputs do with parts of the program (e.g. recognizing null-pointers)</p><p><br></p><h2 id="common-pitfalls-in-using-software-metrics">Common pitfalls in using software metrics</h2><ul><li><strong>Metric in a bubble</strong>: Using a metric without proper interpretation. Recognized by not being able to explain what a given value of a metric means. Can be solved by placing the metric in a context with respect to a goal.</li><li><strong>One-track metric: f</strong>ocusing on only a single metric. Recognized by seeing only one metric (of just a few) on display. Can be solved by adding metrics relevant to the goal.</li><li><strong>Metrics galore: </strong>focusing on too many metrics. Recognized when the team ignores all metrics. Can be solved by reducing the number of metrics used.</li><li><strong>Treating the metric: </strong>making alterations just to improve the value of a metric. Recognized when changes made to the software are purely cosmetic. Can be solved by determining the root cause of the value of a metric.</li></ul>
  </section>

    </section>
  </div>
</body>

</html>