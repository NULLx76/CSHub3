<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>MyBlog</title>
  <link rel="stylesheet" href="https://nullx76.github.io/CSHub3/main.css">
</head>

<body>
  <div class="container">
    <head>
      <h1><a href="/">CSHub.nl</a></h1>
    </head>
    <section class="section">
      
  <section class="page">
  <p>A model learned from data is never the absolute truth</p><ul><li>The data can be wrong, noisy, biased</li><li>The learning bias can be wrong: e.g., maximize accuracy, independence assumptions</li><li>The learning algorithm may not find the optimal model</li><li>The chosen target model can be wrong: e.g., there is no decision tree or neural network</li></ul><p><br></p><p>You should avoid using learned models as black boxes as they are often wrong. It can be essential to know why they are wrong.'</p><p><br></p><h2 id="discrimination">Discrimination</h2><p>Learned models often have some form of discrimination. The default solution is to remove / censor sensitive attributes (race, gender etc).</p><p><br></p><p>This causes <strong>redlining: </strong>determining sensitive attributes from other attributes</p><p><br></p><p>This does not work, we should instead of only <strong>maximizing</strong> accuracy / likelihood learn a model that <strong>maximizes</strong> accuracy, <strong>while minimizing</strong> discrimination.</p><p><br></p><h3 id="1.-preferential-sampling">1. Preferential sampling</h3><ul><li>Randomly draw examples from every sensitive group</li><li><strong>From a discriminated group</strong>: draw more examples with positive labels / draw less examples with negative labels</li><li><strong>From a favored group: </strong>draw less positive labels / draw more examples with negative labels</li></ul><p><br></p><p>We do this until we have a data set with 0 discrimination (we maintain the same fraction of positive/negative labels) and then learn a model from this data.</p><p><br></p><h3 id="2.-different-thresholds">2. Different thresholds</h3><p>Learn a probabilistic model which gives a probability <span class="ql-formula" data-value="P\left(i\right)">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mrow><mo fence="true">(</mo><mi>i</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">P\left(i\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span style="margin-right: 0.13889em;" class="mord mathdefault">P</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord mathdefault">i</span><span class="mclose delimcenter" style="top: 0em;">)</span></span></span></span></span></span>﻿</span> that sample <span class="ql-formula" data-value="i">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.65952em; vertical-align: 0em;"></span><span class="mord mathdefault">i</span></span></span></span></span>﻿</span> belongs to the positive class. </p><p><br></p><p>Use different decision thresholds <span class="ql-formula" data-value="t">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.61508em; vertical-align: 0em;"></span><span class="mord mathdefault">t</span></span></span></span></span>﻿</span> for the sensitive groups such that if <span class="ql-formula" data-value="P\left(i\right)>t">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mrow><mo fence="true">(</mo><mi>i</mi><mo fence="true">)</mo></mrow><mo>&amp;gt;</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">P\left(i\right)&amp;gt;t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span style="margin-right: 0.13889em;" class="mord mathdefault">P</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord mathdefault">i</span><span class="mclose delimcenter" style="top: 0em;">)</span></span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 0.61508em; vertical-align: 0em;"></span><span class="mord mathdefault">t</span></span></span></span></span>﻿</span>, <span class="ql-formula" data-value="i">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.65952em; vertical-align: 0em;"></span><span class="mord mathdefault">i</span></span></span></span></span>﻿</span> is labeled as positive and the fraction of examples labeled as positive is roughly equal for every sensitive group</p><p><br></p><h3 id="3.-different-models">3. Different models</h3><p>Learn a probabilistic model for every sensitive group and do the same as for (2). This actively avoid redlining</p><p><br></p><h3 id="simpsons-paradox">Simpsons paradox</h3><p>Simpsons paradox <span style="color: rgb(34, 34, 34);">is a phenomenon in probability and statistics, in which a trend appears in several different groups of data but disappears or reverses when these groups are combined.</span></p><p><br></p><p><span style="color: rgb(34, 34, 34);">So on the surface it might seem that there is discrimination, while there actually isn't. </span></p><p><br></p><h3 id="conclusion">Conclusion</h3><p><span style="color: rgb(34, 34, 34);">A little discrimination is even OK when it can be explained by other attributes besides race / gender etc. </span>One can use attributes such as education &amp; experience to split the population into subgroups, discrimination in one subgroup is bad, but between subgroups is OK.</p><p><br></p><p>We then:</p><ul><li>Apply preferential sampling on every subgroup</li><li>Ensure that the positive class probability for every subgroup remains the same!</li></ul><p><br></p><p>Note that this is only a <em>possible </em>solution, this is an area of active research. Another measure could be false positive rate. This rate <em>should </em>be about equal between different races / genders.</p><p><br></p><h3 id="affirmative-action">Affirmative action</h3><p>The policy of favoring members of a disadvantaged group who suffer from discrimination (positive discrimination)</p><p><br></p><p>By not blindly trusting statistics/models, modifying their outcome, we apply <strong>affirmative action</strong>!</p><p><br></p><p>Statistics are biased, but applying affirmative action is still considered illegal in many countries</p>
  </section>

    </section>
  </div>
</body>

</html>