<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>MyBlog</title>
  <link rel="stylesheet" href="https://nullx76.github.io/CSHub3/main.css">
</head>

<body>
  <div class="container">
    <head>
      <h1><a href="/">CSHub.nl</a></h1>
    </head>
    <section class="section">
      
  <section class="page">
  <p>MLPs have a few problems:</p><ul><li>They do not scale</li><li>They can't handle translation</li><li>They ignore the spatial structure</li></ul><p><br></p><p>To solve that, we can use a <strong>Convolutional Neural Network (CNN)</strong></p><p><br></p><h1 id="convolutional-neural-network">Convolutional Neural Network</h1><p>We might want to identify whether an image is an <span class="ql-formula" data-value="X">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span style="margin-right: 0.07847em;" class="mord mathdefault">X</span></span></span></span></span>﻿</span> or an <span class="ql-formula" data-value="O">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi></mrow><annotation encoding="application/x-tex">O</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span style="margin-right: 0.02778em;" class="mord mathdefault">O</span></span></span></span></span>﻿</span> by using a CNN. The tricky part is that the input might be transformed from what it originally was:</p><p><img src="https://i.imgur.com/UnBVdWF.png" width="323"></p><p><em>Image taken from video by Brandon Rohrer</em></p><p><br></p><p>We represent this as a grid of <span class="ql-formula" data-value="0">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">0</span></span></span></span></span>﻿</span>'s and <span class="ql-formula" data-value="1">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span>﻿</span>'s and you will see that some parts of the image will have matching elements, but some don't. This gives uncertainty as to what we are dealing with.</p><p><br></p><p>We solve that by applying a couple of techniques:</p><p><br></p><h3 id="filtering">Filtering</h3><p>A CNN can match parts of the images, smaller <strong>features</strong>. So we might distribute match parts of the image: <strong>filtering</strong>. This is done as follows:</p><ol><li>Line up the feature (or <strong>filter / kernel</strong>) and image patch</li><li>Multiply each image pixel by the corresponding feature pixel</li><li>Add them up</li><li>Put the sum in the <strong>feature map</strong>. </li></ol><p><br></p><p>This looks as follows:</p><p><img src="https://miro.medium.com/max/900/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif" width="378"></p><p><em>Image taken from </em><a href="https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2" target="_blank"><em>here</em></a></p><p><br></p><p>Due to the size of our filter (3x3), our feature map and our <strong>receptive field </strong>(the green area) are coincidentally also 3x3. In reality this is performed in 3D, as images are represented as 3D matrices with depth 3 for the 3 color channels. Our filter covers all 3 color channels.</p><p><br></p><p>We perform multiple convolutions on an input, using a different filter resulting in different feature maps. We stack all those feature maps together which become the final output of the <strong>convolution layer</strong>.</p><p><br></p><h3 id="non-linearity">Non-linearity</h3><p>We perform a so called <strong>relu </strong>activation function to our feature maps. So the values in the final feature maps are not actually the sums, but the relu function applied to them. This has been omitted this in the figure above for simplicity. But keep in mind that any type of convolution involves a relu operation, without that the network won’t achieve its true potential.</p><p><br></p><p>This rectified linear activation function is a piecewise linear function that will output the input directly if is positive, otherwise.</p><p><br></p><h3 id="stride-and-padding">Stride and padding</h3><p>We also need to determine how much we want to shift the previously mentioned filters. In our example above, this was <span class="ql-formula" data-value="1">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span>﻿</span>, so the <strong>stride </strong>was 1. This means that we lose the edges. </p><p><br></p><p>As we want to preserve information at the edges of the image, we apply <strong>padding</strong>. We add a padding of zeros or values on the edge, so our feature map still contains the edge values. We usually apply this padding so that the size of our maps doesn't decrease w.r.t. the original image.</p><p><img src="https://miro.medium.com/max/1063/1*W2D564Gkad9lj3_6t9I2PA@2x.gif" width="388"></p><p><em>Image taken from </em><a href="https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2" target="_blank"><em>here</em></a></p><p><br></p><h3 id="pooling">Pooling</h3><p>Another trick we have is <strong>pooling</strong>, which helps us to reduce the dimensionality:</p><ol><li>Pick a window size (usually 2x2 pixels or 3x3 pixels)</li><li>Pick a stride (usually 2 pixels)</li><li>Walk your window across your filtered images</li><li>From each window, take the maximum value (most common option, there is also average pooling)</li></ol><p><br></p><p><img src="https://miro.medium.com/max/1172/1*ReZNSf_Yr7Q1nqegGirsMQ@2x.png" width="404"></p><p><em>Image taken from </em><a href="https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2" target="_blank" style="background-color: rgb(255, 255, 255);"><em>here</em></a></p><p><br></p><p><span style="background-color: rgb(255, 255, 255);">There is no overlap here because of the chosen values for the window size and stride. </span>We perform pooling for our entire <strong>convolution layer.</strong></p><p><br></p><p>Because pooling doesn't care about <em>where</em><strong> </strong>the max value is located, it helps reduce the impact of the previously mentioned transformations.</p><h3 id=""><br></h3><h3 id="fully-connected-layer">Fully connected layer</h3><p>For our final layer, we use a <strong>fully connected</strong> layer. We take our 3D layers and flatten them into a 1D vector. This is our FC layer.</p><p><br></p><p>Every value in the FC layer gets a vote on what the answer is going to be. The votes depend on how strongly a value predicts the <span class="ql-formula" data-value="X">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span style="margin-right: 0.07847em;" class="mord mathdefault">X</span></span></span></span></span>﻿</span> or <span class="ql-formula" data-value="O">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi></mrow><annotation encoding="application/x-tex">O</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span style="margin-right: 0.02778em;" class="mord mathdefault">O</span></span></span></span></span>﻿</span>. When we get a new image, we put it through the same series of transformations and apply voting based on the <strong>learned weights</strong>. We learn these weights with backpropagation with gradient descent, just as with the MLP!</p><p><br></p><p>In the following image you can see that we have 4 layers of convolution &amp; pooling layers and 2 fully connected layers:</p><p><br></p><p><img src="https://miro.medium.com/max/2175/1*uUYc126RU4mnTWwckEbctw@2x.png" width="534"></p><p><em>Image taken from </em><a href="https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2" target="_blank" style="background-color: rgb(255, 255, 255);"><em>here</em></a></p><p><br></p><h1 id="recurrent-neural-network">Recurrent Neural Network</h1><p><em>Adapted from </em><a href="https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce" target="_blank" style="background-color: rgb(255, 255, 255);"><em>here</em></a></p><p><br></p><p>Recurrent neural networks, also known as RNNs, are a class of neural networks that allow previous outputs to be used as inputs while having hidden states. RNNs are designed to take a series of input with no predetermined limit on size. A single input item from the series is related to others and likely has an influence on its neighbors. So we need something that captures this relationship across inputs meaningfully.</p><p><br></p><p>A RNN remembers the past and it’s decisions are influenced by what it has learnt from the past. Basic feed forward networks “remember” things too, but they remember things they learnt during training. For example, an image classifier learns what a “1” looks like during training and then uses that knowledge to classify things in production.</p><p><br></p><p>While RNNs learn similarly while training, in addition, they remember things learnt from prior input(s) while generating output(s). It’s part of the network. RNNs can take one or more input vectors and produce one or more output vectors and the output(s) are influenced not just by weights applied on inputs like a regular NN, but also by a “hidden” state vector representing the context based on prior input(s)/output(s). So, the same input could produce a different output depending on previous inputs in the series:</p><p><img src="https://miro.medium.com/max/1590/1*aIT6tmnk3qHpStkOX3gGcQ.png" width="363"></p><p><em>Image taken from </em><a href="https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce" target="_blank"><em>here</em></a></p><p><br></p><h2 id="parameter-sharing">Parameter sharing</h2><p>In an MLP, multiple different weights are applied to the different parts of an input item generating a hidden layer neuron, which in turn is transformed using further weights to produce an output. In a RNN, we seem to be applying the same weights over and over again to different items in the input series.</p><p><br></p><p>In an RNN we use a technique called <strong>sharing parameters</strong> across inputs. If we don’t share parameters across inputs, then it becomes like a vanilla neural network where each input node requires weights of their own. This introduces the constraint that the length of the input has to be fixed and that makes it impossible to leverage a series type input where the lengths differ and is not always known. Because we have less weights, we lose some versatility.</p><p><br></p><p>But what we seemingly lose in value here, we gain back by introducing the “hidden state” that links one input to the next. The hidden state captures the relationship that neighbors might have with each other in a serial input and it keeps changing in every step, and thus effectively every input undergoes a different transition!</p><p><br></p><h3 id="word-prediction">Word prediction</h3><p>We can use a RNN to predict words, using for example word2vec. This transforms a word into a vector or numbers, called an <strong>embedding. </strong>We can pass an aggregation of embeddings of the previous words the prediction of the next word, which can give a very accurate result.</p><p><br></p><h3 id="machine-translation">Machine translation</h3><p><span style="color: rgb(36, 41, 46);">A machine translation model is similar to a language model except it has an encoder network placed before. For this reason, it is sometimes referred as a conditional language model.</span></p><p><br></p><h3 id="lstm">LSTM</h3><p>An LSTM introduces changes to how we compute outputs and hidden state using the inputs.</p><p><br></p><p>In a vanilla RNN, the input and the hidden state are simply passed through a single <span class="ql-formula" data-value="\tanh">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>tanh</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\tanh</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mop">tanh</span></span></span></span></span>﻿</span> layer. LSTM (Long Short Term Memory) networks improve on this simple transformation and introduces additional gates and a cell state, such that it fundamentally addresses the problem of keeping or resetting context, across sentences and regardless of the distance between such context resets. </p><p><br></p><p>There are variants of LSTMs including GRUs that utilize the gates in different manners to address the problem of long term dependencies.</p><p><br></p>
  </section>

    </section>
  </div>
</body>

</html>