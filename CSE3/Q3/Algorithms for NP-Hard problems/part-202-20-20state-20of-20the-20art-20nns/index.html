<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>MyBlog</title>
  <link rel="stylesheet" href="https://nullx76.github.io/CSHub3/main.css">
</head>

<body>
  <div class="container">
    <head>
      <h1><a href="/">CSHub.nl</a></h1>
    </head>
    <section class="section">
      
  <section class="page">
  <h2 id="alphago">AlphaGo</h2><p>Go is a <span class="ql-formula" data-value="19\times19">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>19</mn><mo>×</mo><mn>19</mn></mrow><annotation encoding="application/x-tex">19\times19</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mord">9</span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord">9</span></span></span></span></span>﻿</span> board game where player alternately place stones and capture stones by encirclement. The player with the most surrounded territory and captured stones wins.</p><p><br></p><p>AlphaGo does the following to play Go:</p><ol><li>Behavior cloning from human tournaments</li><li>Refinement through self-play</li><li>Different networks for different jobs</li></ol><p><br></p><p>Here you can see the number of points per type of network:</p><p><img src="https://i.imgur.com/d1I0v3r.png" width="262"></p><p>But, it turns out you don't the behavior cloning and it can discover even better moves without behavior cloning. They did this by:</p><ul><li>Combining the policy and value heuristic into one network</li><li>Got rid of reinforcement learning loss and used classification loss (if you run this for long enough this loss is more stable)</li><li>Got rid of rollouts</li><li>More compute power</li></ul><p><br></p><h2 id="muzero">MuZero</h2><p>Now, AlphaZero requires the game rules, but we may be able to learn those too. It works as follows:</p><ul><li>Take the possible future actions <span class="ql-formula" data-value="k">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span style="margin-right: 0.03148em;" class="mord mathdefault">k</span></span></span></span></span>﻿</span> that you will take to go to a leaf</li><li>Compute the important quantities with a giant NN</li><li class="ql-indent-1"><span class="ql-formula" data-value="p_t^k">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>p</mi><mi>t</mi><mi>k</mi></msubsup></mrow><annotation encoding="application/x-tex">p_t^k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1.096108em; vertical-align: -0.247em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -2.4530000000000003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span style="margin-right: 0.03148em;" class="mord mathdefault mtight">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span></span></span></span></span>﻿</span>: the probability over all actions that you could take, using a standard loss function</li><li class="ql-indent-1"><span class="ql-formula" data-value="v_t^k">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>v</mi><mi>t</mi><mi>k</mi></msubsup></mrow><annotation encoding="application/x-tex">v_t^k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1.096108em; vertical-align: -0.247em;"></span><span class="mord"><span style="margin-right: 0.03588em;" class="mord mathdefault">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -2.4530000000000003em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span style="margin-right: 0.03148em;" class="mord mathdefault mtight">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span></span></span></span></span>﻿</span>: the value (state) heuristic (probability a game will be won), using standard mean squared regression against the sum of the rewards that the system produced</li><li class="ql-indent-1"><span class="ql-formula" data-value="r_t^k">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>r</mi><mi>t</mi><mi>k</mi></msubsup></mrow><annotation encoding="application/x-tex">r_t^k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1.096108em; vertical-align: -0.247em;"></span><span class="mord"><span style="margin-right: 0.02778em;" class="mord mathdefault">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -2.4530000000000003em; margin-left: -0.02778em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span style="margin-right: 0.03148em;" class="mord mathdefault mtight">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span></span></span></span></span>﻿</span>: the reward in the leaf</li></ul><p><br></p><p>Or mathematically:</p><p><img src="https://i.imgur.com/aCylFiK.png" width="665"></p><p><br></p><p>This actually works really well, and outperforms AlphaZero.</p><p><br></p><h2 id="alphastar">AlphaStar</h2><p>AlphaStar learns how to play StarCraft on specific maps using complex neural architectures and RL algorithms.</p><p><br></p><p>Its architecture looks as such:</p><p><img src="https://i.imgur.com/xDo1jGt.png" width="653"></p><p>Note that there are a lot of inputs into the network, all of which are inputs that you could have.</p><p><br></p><p>Its league consists of:</p><ul><li><strong>Main agents vs self + league</strong></li><li><strong>Main exploiters vs main agents: </strong>actively learning agents to find weakness in the current agent's strategies</li><li><strong>League exploiters vs league:</strong> to find the weak spots in the strategies that are currently played in the league</li></ul>
  </section>

    </section>
  </div>
</body>

</html>