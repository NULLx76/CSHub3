<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>MyBlog</title>
  <link rel="stylesheet" href="https://nullx76.github.io/CSHub3/main.css">
</head>

<body>
  <div class="container">
    <head>
      <h1><a href="/">CSHub.nl</a></h1>
    </head>
    <section class="section">
      
  <section class="page">
  <h1 id="user-evaluation">User Evaluation</h1><p>Here, you evaluate with the people the system is aimed at. You can do this in different stages of the design cycle:</p><ul><li><strong>Design: </strong>based on ideas/sketches</li><li><strong>Prototype: </strong>based on working system</li><li><strong>Use: </strong>based on daily use in the real world</li></ul><p><br></p><h2 id="when?">When?</h2><p>When you evaluate determines the way you evaluate:</p><ul><li><strong>Design stage</strong> (you have: ideas)</li><li class="ql-indent-1">Users (e.g. focus groups, value-sensitive design)</li><li class="ql-indent-1">Experts (cog. walkthrough)</li><li><strong>Prototype stage</strong> (you have: protoype)</li><li class="ql-indent-1">Users (e.g. lab experiments, cooperative)</li><li class="ql-indent-1">Experts (e.g. heuristic, cog. walkthrough)</li><li><strong>Deployment stage</strong> (you have: product)</li><li class="ql-indent-1">Users (e.g. field experiments)</li><li class="ql-indent-1">Data analytics</li></ul><p><br></p><h2 id="non-controlled-experiments">Non-controlled experiments</h2><p><img src="https://i.imgur.com/ppU4c4K.png" width="551"></p><h3 id="cooperative-evaluation">Cooperative Evaluation</h3><p>Cooperative evaluation involves users a little bit earlier, so no full working prototype is needed. The idea is that it's cooperative because the participants work only in cooperation with an evaluator (experimenter). </p><p><br></p><p>It takes a lot of time because you need 1-1 interaction with the user, so it may not always be economical.</p><p><br></p><p>It consists of the following tasks:</p><p><img src="https://i.imgur.com/kYcYQhl.png" width="482"></p><p>An example of how such an evaluation could go:</p><p><strong>E: Yeah, this box over here</strong></p><p>U: No references selected. Ah.</p><p><strong>E: This is a message box yeah. What do you think no references selected means?</strong></p><p>U: Well I would have assumed, that it couldn’t find it and one of the reasons that maybe it couldn’t find it is because it’s not in the library.</p><p><strong>E: Okay. Well no references selected definitely means it hasn’t found it.</strong></p><p>U: And I don’t think I spelt it wrongly.</p><p><br></p><p>Note that you don't try to interpret the actions, but write down the actual actions that were taken.</p><p><br></p><h3 id="participatory-heuristic-evaluation">Participatory heuristic evaluation</h3><p>This is basically the same as expert evaluation, but then with end users: users independently evaluate the user interface to determine whether it confirms to established usability principles (heuristics). See lecture 3 about design.</p><p><br></p><p>The users should actually know what the process looks like and what the heuristics mean.</p><p><br></p><h3 id="co-discovery">Co-discovery</h3><p>Here you have a group of end users to capture their first impression. It's less structured, so you need to have a more worked out design.</p><p><br></p><p>The users are meant to <strong>think aloud</strong> and let them <strong>talk to each other</strong>: what am I looking at, what am I thinking, doing or feeling.</p><p><br></p><p>The experimenters record and transcribe what was said and what everyone was doing when they were saying something. If it's hard to keep people talking, you may need to prompt them.</p><p><br></p><h3 id="interviews">Interviews</h3><p>For open and complicated questions, an interview is suitable. You may prepare many questions, or only topics or opening questions (to start a conversation).</p><p><br></p><p>It should not be too long, to make sure users don't get tired.</p><p><br></p><p>The key is to ask for conformation before writing down answers.</p><p><br></p><h2 id="controlled-experiments">Controlled experiments</h2><p>In controlled experiments, you evaluate one particular aspect of the system. You may compare different options to determine which is better.</p><p><br></p><p><strong>Advantages:</strong></p><ul><li>People are actually using the system: if you want to study how a system is used or what effect it has, you need people to actually use it</li><li>It's a good way to test with end-users</li></ul><p><br></p><p><strong>Disadvantages:</strong></p><ul><li>You need a workable system: if you evaluate a system that is not finished, your results may be affected</li><li>Takes time and resources: to get the people and to get the system to a right state</li></ul><p><br></p><p><strong>Lab study: </strong>in a lab study you bring people into a <strong>controlled environment</strong>. This can be a room with many computers or even online. It is easy, cheap and does not require full employment</p><p><strong>Field study: </strong>here you study users in their <strong>daily life. </strong>This is more realistic and you can see how a user uses your product in a normal context. However, this is costly to set up.</p><p><br></p><h3 id="variables">Variables</h3><p>There are three types of variables: independent, dependent and confounding variables.</p><p><img src="https://i.imgur.com/M8a6Mo4.png" width="416"></p><p><strong>Independent: </strong>the thing that you will manipulate or change, i.e. the experimental condition</p><p><strong>Dependent: </strong>the thing you expect to change, i.e. the thing that you measure</p><p><strong>Confounding: </strong>things that might influence your result other than independent variables. They cause your result not to accurately reflect the actual relationship between the independent and dependent variable.</p><p><br></p><p>For example, if you are studying the chance of Down syndrome in 1st, 2nd and 3rd children, the age of the mother is confounding factor:</p><ul><li>Correlated with 1st, 2nd or 3rd child</li><li>Age of mother influences chance of Down syndrome</li></ul><p><br></p><p>It is good to measure possible confounding factors, so you can control them. However, you should only measure them if you have a good rationale for them.</p><p><br></p><p><strong>P-hacking: </strong>Performing many tests on many data points and only reporting those that return significant</p><p><br></p><h3 id="design">Design</h3><p>There are two types of controlled experiments: </p><ul><li><strong>Within subject: </strong>the same person tests all the conditions (i.e., all the user interfaces).</li><li><strong>Between subject: </strong>different people test each condition, so that each person is only exposed to a single user interface</li></ul><p><img src="https://i.imgur.com/0Tdo99d.png" width="498"></p><p><img src="https://i.imgur.com/FClCeU1.png" width="571"></p><p><br></p><h3 id="procedure">Procedure</h3><ol><li>Decide on independent &amp; dependent variables</li><li>Look at literature for possible confounding variables</li><li>Decide on experimental design (within-between subject)</li><li>Decide on who will participate &amp; how many people\</li><li>Write out full procedure</li><li>Decide on how you will analyze data</li><li>Make list of possible outcomes &amp; meaning</li><li>Write ethics proposal</li><li>Do pilot experiment</li></ol><p><br></p><h2 id="evaluation-methods">Evaluation methods</h2><h3 id="empirical-vs-analytic-evaluation">Empirical vs analytic evaluation</h3><p>In <em>analytic evaluations</em>, people reason about the usability <strong>without involving users </strong>(e.g. cognitive walkthrough, simulations, heuristic evaluations). Also called <strong>inspection methods</strong>.</p><p><br></p><p><em>Empirical evaluations</em> <strong>involves users</strong> in the evaluation (e.g. questionnaire, interview, observations, experiments, context inquiry)</p><p><br></p><h3 id="predictive-vs-interpretive-evaluation">Predictive vs interpretive evaluation</h3><p><strong>Predictive evaluation</strong>: low-cost evaluation methods which predict the usability of a system from a model, specification or early prototype (e.g. heuristic evaluation).</p><p><br></p><p><strong>Interpretive evaluation</strong> helps designers to understand better how users use a system in their natural environment and how the use of these systems integrates with other activities (e.g. ethnography).</p><p><br></p><h3 id="formative-vs-summative-evaluation">Formative vs summative evaluation</h3><p><strong>Formative evaluation</strong> is done during the construction of a product to find usability obstacles for improvement and has a more explorative and qualitative nature.</p><p><br></p><p><strong>Summative evaluation</strong> is used to assess or compare the level of usability achieved in an entire product when finished (formal experimental design, including statistical analyses).</p><p><br></p><h3 id="controlled/experimental-vs.-field-evaluation">Controlled/Experimental vs. Field evaluation</h3><p>In <strong>controlled lab study</strong>, users use the system to evaluate a system in a controlled environment</p><p><br></p><p><strong>Field study</strong>: a study that is done in a natural setting</p><p><br></p><h1 id="metrics-and-measures">Metrics and measures</h1><h3 id="subjective-measurements"><strong>Subjective measurements</strong></h3><p>Depends on the person, what people say, e.g.</p><ul><li>Questionnaires</li><li>Interviews</li></ul><p><br></p><h3 id="objective-measurements"><strong>Objective measurements</strong></h3><p>Physiological measures and behavior (what people do), e.g.</p><ul><li>Heart rate (physiological)</li><li>Skin conductance (physiological)</li><li>How much they click (behavioral)</li><li>How long they use something (behavioral)</li></ul><p><br></p><h3 id="questionnaire">Questionnaire</h3><p>Questionnaires are good for measuring opinions/thoughts &amp; measuring one specific thing.</p><p><br></p><p>However, it is hard to make one. You should ask yourself what you are really measuring and whether multiple questions are measuring the same thing (we don't want this).</p><p><br></p><p>There are some questions you can ask yourself:</p><ul><li>Are the questions clear / neutral / appropriate?</li><li>Will you ask open or closed questions?</li><li>Is the answer scale clear?</li><li>Am I measuring what I think I am measuring (validity)?</li></ul><p><br></p><p>Afterwards you need to do some analysis. Important, decide before you start how you will do the analysis! Don’t put questions in because they seem interesting. Consider how they relate to your overall question.</p><p><br></p><h2 id="analysis">Analysis</h2><p>You always report the mean and the standard deviation. You need both to show you how diverse your data is.</p><p><br></p><h3 id="comparing-means">Comparing means</h3><p>The thing you do most often is comparing means is using a <strong>T-test:</strong></p><ul><li>Independent means (e.g. group A vs group B)</li><li>Paired means (e.g. before &amp; after)</li></ul><p><br></p><p>We use a null hypothesis <strong>(H0): </strong>no difference between the means. You can only do this if there is a normal distribution in your means.</p><p>Given this null hypothesis (no difference), how likely is it to observe the data that we have. If <span class="ql-formula" data-value="P<0.05">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>&amp;lt;</mo><mn>0.05</mn></mrow><annotation encoding="application/x-tex">P&amp;lt;0.05</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72243em; vertical-align: -0.0391em;"></span><span style="margin-right: 0.13889em;" class="mord mathdefault">P</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">5</span></span></span></span></span>﻿</span>, it is significant -&gt; we do have a difference.</p><p><br></p><p>If there are more than 2 means, we use the ANOVA test: analysis of variance.</p><p><br></p><h3 id="correlation">Correlation</h3><p>Correlation is statistical association between 2 variables. <strong>Correlation is not causation!</strong></p><p><img src="https://i.imgur.com/JN7RV9S.png" width="482"></p><p><br></p><h2 id="people">People</h2><p>When selecting people which participants you need, you need to create two criteria:</p><ul><li><strong>Inclusion criteria: </strong>things all participants need to have</li><li><strong>Exclusion criteria: </strong>things no participant can have</li></ul><p><br></p><p>The amount of participants you need depends on:</p><ul><li>Experimental design (<em>within subjects</em> requires less than <em>between subjects)</em></li><li>Statistical analysis</li><li>Statistical power &amp; effect size (how strong your test is to detect an effect of what size) </li></ul><p><br></p><p>However, you also need to think about the practice: what is reasonably possible?</p><p><br></p><h2 id="reporting">Reporting</h2><p>See <a href="https://cshub.nl/post/958979804#evaluation" target="_blank">evaluation</a> for what you need to report (the table).</p>
  </section>

    </section>
  </div>
</body>

</html>