+++
title = "Bias "
date = 2019-10-02
+++
<h2 id="bias">Bias</h2><p>A bias is a <strong>preference or inclination </strong>for or against something, which can be positive, negative or neutral. It is often accompanied by a refusal to consider the merits of other points of view.</p><p><br></p><h3 id="prejudice">Prejudice</h3><p>A prejudice is an <strong>assumption </strong>made without adequate knowledge. It is most commonly used to refer to a preconceived judgement toward a person or a group of people because of a personal or specific characteristic.</p><p><br></p><p>It is usually resistant to rational influence</p><p><br></p><h3 id="discrimination">Discrimination</h3><p>Discrimination are <strong>actions </strong>taken based on a prejudice. For example treating a person or group of persons based solely on their membership of a certain group or category.</p><p><br></p><h3 id="implicit-bias">Implicit bias</h3><p>The implicit bias are expectations based on learned coincidences, which unknowingly affect everyday perceptions, judgment, memory, and behavior, like a subconscious thought.</p><p><br></p><p>An implicit bias might lead to discrimination.</p><p><br></p><h3 id="explicit-bias">Explicit bias</h3><p>The explicit bias is informed by our&nbsp;implicit bias but is also at least in part a conscious choice. For example, <strong>walking on the other </strong>(conscious decision) side of the street when you see a <strong>scary-looking person </strong>(implicit bias)</p><p><br></p><h2 id="bias-in-ml">Bias in ML</h2><p>AI systems or ML techniques are not inherently “bad” nor turn “bad” by themselves, an important source for bias is the training set. This can happen because some features might seem correlated, even though they aren't, thus leading to incorrect (biased) conclusions.</p><p><br></p><p>Other sources could be:</p><ul><li>Lack of diversity in ML developers</li><li>Implicit human biases in our culture</li><li>Evil programmers</li></ul><p><br></p><h3 id="fairness">Fairness</h3><p>The <strong>algorithm </strong>component can be unbiased and fair, if there is bias in the data used to make a decision, the decision itself may be biased.</p><p><br></p><p><strong>Fairness</strong>: “The absence of bias or discrimination on specific realms”</p><p><br></p><p>But definition depend on many aspects, such as domain, context, and social constructs. There are 3 ways of quantifying fairness in ML:</p><ul><li><strong>Data vs. Model</strong>: Fairness can be measured at different stages in a machine learning pipeline. Specifically, fairness can be quantified in the training dataset or in the learned model of a machine learning solution.</li></ul><p><br></p><ul><li><strong>Individual vs. Group</strong>: Another friction that defines fairness in machine learning algorithms is Group vs. Individual. </li><li class="ql-indent-1">Group fairness; in its broadest sense, partitions a population into groups defined by protected attributes and seeks for some statistical measure to be equal across groups.</li><li class="ql-indent-1">Individual fairness; in its broadest sense, seeks for similar individuals to be treated similarly.</li></ul><p><br></p><ul><li><strong>WAE vs. WYSIWYG</strong>: </li><li class="ql-indent-1">We are all equal (WAE) defines fairness as an equal distribution of skills and opportunities among the participants in a machine learning task. </li><li class="ql-indent-1">The what you see is what you get (WYSIWYG) perspective reflects that the observations reflect ability with respect to the task. </li><li class="ql-indent-1">In a scenario that uses SAT scores as a feature for predicting success in college, the WYSIWYG worldview says that the score correlates well with future success and that there is a way to use the score to correctly compare the abilities of applicants. In contrast, the WAE worldview says that the SAT score may contain structural biases so its distribution being different across groups should not be mistaken for a difference in distribution in ability.</li></ul><p><br></p><h3 id="debiasing">Debiasing</h3><p>To debias the three previously mentioned measurements, we can do the following:</p><ul><li><strong>Data vs. Model</strong>: Check the distribution of class labels in training set. If there are more examples in group <span class="ql-formula" data-value="A">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault">A</span></span></span></span></span>﻿</span> than in <span class="ql-formula" data-value="B">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span style="margin-right: 0.05017em;" class="mord mathdefault">B</span></span></span></span></span>﻿</span>, equalize the distribution in the training set</li></ul><p><br></p><ul><li><strong>Individual vs. Group: </strong></li><li class="ql-indent-1">Group fairness; be aware of correlations of variables with other variables that the algorithm uses, e.g., surnames with geographical census data</li><li class="ql-indent-1">Individual fairness; 2 individuals on either side of the line are very similar but different outcome</li></ul><p><br></p><ul><li><strong>WAE vs. WYSIWYG</strong>: Can be debiased by a smart implementation of the algorithm</li></ul><p><br></p><h3 id="now-vs-then">Now vs then</h3><p>In the "old" days, a request for a loan was handled per person, where the person requesting the loan could explain the case and the person deciding on the loan would use rules and common sense to make a decision. This is called "Street-level bureaucracy"</p><p><br></p><p>Nowadays, everything is handled by an algorithm, which has no room for personal story or bending the rules, which gives us "System-level bureaucracy". This brings the system from a personal, case-by-case decision system to an automatic system which is controlled by programmers. Only in cases when there is an appeal a person will look at the case.</p><p><br></p><p>But this begs the question, who is and who should be responsible for making these decisions? This is done by both data and decision-making algorithms, which use the data.</p><p><br></p><p>This data can come from many sources, like individuals or other companies and it is used in formulas which are used to make decisions. The answer to our question would then be that decisions are based on data from other organisations and are made by the programmers of the software. But, these programmers don't see the individuals that are affected by the decisions, don't see individual cases and aren't decision-making experts.</p><p><br></p><p>In conclusion:</p><ul><li>Automatic systems:</li><li class="ql-indent-1">Are more efficient</li><li class="ql-indent-1">Process cases much faster</li><li class="ql-indent-1">Eliminate biases in street-level bureaucrats</li><li>But:</li><li class="ql-indent-1">They provide little room to correct errors</li><li class="ql-indent-1">IT determines which cases are to be dealt with automatically</li><li class="ql-indent-1">Important roles for programmers with little background knowledge on decision making</li><li class="ql-indent-1">Cultural bias might enter the algorithms</li><li class="ql-indent-1">Important roles for parameters in algorithms</li></ul><p><br></p><p>So, new ethical rules are needed and building fairness and non-discriminatory behaviour into AI models is not only a matter of technological advantage but of social responsibility</p><p><br></p>