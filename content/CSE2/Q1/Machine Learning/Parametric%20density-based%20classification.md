+++
title = "Parametric density-based classification"
date = 2019-09-16
+++
<h1 id="discriminative-vs-generative-models"><span>Discriminative vs generative models</span></h1><p><span style="background-color: transparent;">By minimizing the risk or error probability, we split the feature space into <span class="ql-formula" data-value="m">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">m</span></span></span></span></span>﻿</span> regions (for a task with <span class="ql-formula" data-value="m">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">m</span></span></span></span></span>﻿</span> classes). If regions <span class="ql-formula" data-value="R_i">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">R_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span style="margin-right: 0.00773em;" class="mord mathdefault">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.31166399999999994em;"><span class="" style="top: -2.5500000000000003em; margin-left: -0.00773em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>﻿</span> and <span class="ql-formula" data-value="R_j">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">R_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord"><span style="margin-right: 0.00773em;" class="mord mathdefault">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.5500000000000003em; margin-left: -0.00773em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span style="margin-right: 0.05724em;" class="mord mathdefault mtight">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>﻿</span> happen to be </span><span>contiguous</span><span style="background-color: transparent;">, then they are separated by a decision surface in the feature space. This surface is where </span><span><span class="ql-formula" data-value="p\left(\omega_i\mid x\right)=p\left(\omega_j\mid x\right)">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mrow><mo fence="true">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mo>∣</mo><mi>x</mi><mo fence="true">)</mo></mrow><mo>=</mo><mi>p</mi><mrow><mo fence="true">(</mo><msub><mi>ω</mi><mi>j</mi></msub><mo>∣</mo><mi>x</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">p\left(\omega_i\mid x\right)=p\left(\omega_j\mid x\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span style="margin-right: 0.03588em;" class="mord mathdefault">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.31166399999999994em;"><span class="" style="top: -2.5500000000000003em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mord mathdefault">x</span><span class="mclose delimcenter" style="top: 0em;">)</span></span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 1.036108em; vertical-align: -0.286108em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span style="margin-right: 0.03588em;" class="mord mathdefault">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.5500000000000003em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span style="margin-right: 0.05724em;" class="mord mathdefault mtight">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mord mathdefault">x</span><span class="mclose delimcenter" style="top: 0em;">)</span></span></span></span></span></span>﻿</span>.</span></p><p><br></p><p><span style="background-color: transparent;">Calculating these surfaces directly via the </span><span>PDFs </span><span style="background-color: transparent;">is called the </span><strong style="background-color: transparent;">generative approach</strong><span style="background-color: transparent;">.</span></p><p><br></p><p><span style="background-color: transparent;">Often, the PDFs are complicated and it may be preferable to compute the decision surfaces directly by means of alternative costs. This uses discriminant functions and is called the </span><strong style="background-color: transparent;">discriminative approach.</strong></p><p><br></p><p><span>In different words:</span></p><ul><li><span>A </span><strong>discriminative</strong><span> model:</span></li><li class="ql-indent-1"><span>models the decision boundaries between classes. Easier said: a discriminative model is good for modelling the </span><strong>differences </strong><span>of classes</span></li><li class="ql-indent-1"><span>learns the </span><strong>conditional </strong><span>probability distribution <span class="ql-formula" data-value="p\left(\omega_i\mid x\right)">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mrow><mo fence="true">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mo>∣</mo><mi>x</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">p\left(\omega_i\mid x\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span style="margin-right: 0.03588em;" class="mord mathdefault">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.31166399999999994em;"><span class="" style="top: -2.5500000000000003em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mord mathdefault">x</span><span class="mclose delimcenter" style="top: 0em;">)</span></span></span></span></span></span>﻿</span></span></li></ul><p><br></p><ul><li><span>A </span><strong>generative</strong><span> model:</span></li><li class="ql-indent-1"><span>models the distribution of each class</span></li><li class="ql-indent-1"><span>learns the </span><strong>joint </strong><span>probability distribution <span class="ql-formula" data-value="p\left(x,\ \omega_i\right)">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mrow><mo fence="true">(</mo><mi>x</mi><mo separator="true">,</mo><mtext>&nbsp;</mtext><msub><mi>ω</mi><mi>i</mi></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">p\left(x,\ \omega_i\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="mspace">&nbsp;</span><span class="mord"><span style="margin-right: 0.03588em;" class="mord mathdefault">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.31166399999999994em;"><span class="" style="top: -2.5500000000000003em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter" style="top: 0em;">)</span></span></span></span></span></span>﻿</span> (how likely is it to encounter the features in each class)</span></li></ul><h1 id=""><br></h1><h1 id="parametric-density-based-classification"><span style="background-color: transparent;">Parametric density-based classification</span></h1><h2 id="density-estimation"><span>Density estimation</span></h2><h3 id="histogram-based-density-estimation">Histogram-based density estimation</h3><p>The easiest way of estimating the density is by using a histogram. The downside to this is that this requires a lot of samples (both in the amount of objects and the amount of repetitions) in order to generate a reliable reading.</p><p><br></p><p>Terminology:</p><ul><li><strong>Parameters: </strong>the estimates of the true distribution parameters, e.g. mean, standard deviation etc.</li><li><strong>Dimension of data: </strong>the amount of features we wish to estimate</li><li><strong>Sample point:</strong> points of data in our dataset</li></ul><p><br></p><p>Take 50 parameters, then for 1 dimensional (amount of features) data we would need about <span class="ql-formula" data-value="1000">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1000</mn></mrow><annotation encoding="application/x-tex">1000</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span></span></span></span></span>﻿</span> sample points. For <span class="ql-formula" data-value="p">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span></span>﻿</span>-dimensional data, you will need <span class="ql-formula" data-value="1000^p">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>100</mn><msup><mn>0</mn><mi>p</mi></msup></mrow><annotation encoding="application/x-tex">1000^p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.664392em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span></span></span></span></span></span></span></span></span>﻿</span> sample points. This is unworkable for <span class="ql-formula" data-value="p>2">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>&amp;gt;</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">p&amp;gt;2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7335400000000001em; vertical-align: -0.19444em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span></span></span></span></span>﻿</span> </p><h3 id=""><br></h3><h2 id="curse-of-dimensionality">Curse of Dimensionality</h2><p>Intuitively using more measurements should give more information about the outcome to predict. But as we never know the real densities, we will have to estimate it. Thus the number of parameters to estimate increases with the number of features. To estimate well, we need more objects.</p><p><br></p><p>Consequence: there is an optimal number of measurements to use, which minimizes the test error:</p><p><img src="https://i.imgur.com/0WbvPoK.png" width="394"></p><p><em>Image taken from slides by David Tax</em></p><p><br></p><h3 id="parametic-models">Parametic models</h3><ul><li><strong>Parametric</strong>: assume simple global model, e.g. Gaussian, and estimate its parameters</li><li><strong>Non-parametric</strong>: assume a simple local model, e.g. uniform or Gaussian</li></ul><p><br></p><p>We use Gaussian because it is easy to estimate its parameters using maximum likelihood and the distribution comes close to reality.</p><p><br></p><p><strong>Multivariate Guassian: </strong>if we have a multi dimensional Gaussian distribution, we need a square <strong>covariance matrix</strong>. The diagonal elements stand for the amount of spread in the both features. The off-diagonal can be seen as the correlation coefficients of the features. For example:</p><p><img src="https://i.imgur.com/muDlRso.png" width="466"></p><p><em>Image taken from slides by David Tax</em></p><p><br></p><p>But we only have samples, not the true Gaussian distribution, so we need to use maximum likelihood estimators for the mean and the covariance matrix.</p><p><br></p><div style="white-space: normal;" class="markdown-body"><p>For the mean we can take the average of each element in the feature vector. For the covariance matrix we can use the following: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi mathvariant="normal">Σ</mi><mo>^</mo></mover><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><mi>μ</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><mi>μ</mi><mo>^</mo></mover><msup><mo stretchy="false">)</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\hat{\Sigma}={1\over n}\sum_{i=1}^n (x_i-\hat{\mu})(x_i-\hat{\mu})^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9467699999999999em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9467699999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">Σ</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">^</span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">μ</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mclose">)</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">μ</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>. An example on estimating the mean and covariance matrix can be found in examples.</p>
</div><p><br></p><h2 id="related-topics">Related topics</h2><h3 id="properties">Properties</h3><p>A projection of a high-dimensional Gaussian is itself again Gaussian.</p><p><br></p><h3 id="mixture-modelling">Mixture modelling</h3><p>Sometimes a dataset isn't very Gaussian. We can:</p><ul><li>Assume model: e.g. Gaussian</li><li>Estimate mean <span class="ql-formula" data-value="\mu">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathdefault">μ</span></span></span></span></span>﻿</span> and covariance <span class="ql-formula" data-value="\Sigma">﻿<span contenteditable="false"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord">Σ</span></span></span></span></span>﻿</span> from data</li></ul><p><br></p><p>Another option is by using multiple Gaussian curves and estimating to which curve a data point belongs (<strong>mixture models</strong>), though estimating parameters for these mixture models s quite hard.</p>